\section{Experimental Evaluation}

We now evaluate the models developed in NetLearner on (1) the 5-class NSL-KDD task and (2) the 2-class UNSW-NB15 task using the following metrics.

\textbf{Accuracy} is the percentage of correctly classified connections
    over the total number of connections in the dataset:
    \begin{align}
        A = \frac{\text{Correct Predictions}}{\text{Number of Records}}
    \end{align} 
    Accuracy is not suitable for evaluating biased datasets where the number
    of records of one class is extremely larger than the number of
    records of another class.
    In NSL-KDD dataset, the number of available U2R records (i.e., 67)
    is two orders of magnitude less than the number of records in other classes
    (i.e., 9711, 7458, 2887 and 2121).
    Therefore, we also consider the precision and recall.

\textbf{Precision} is the percentage of the correctly classified positives over
    the total number of positives predicted by the classifier:
            \begin{align}
                P = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
            \end{align}

\textbf{Recall} is the percentage of the correctly classified positives over
    the total number of relevant elements:
            \begin{align}
                R = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
            \end{align}
%The weight for each class is determined by its proportion in the test dataset,
%namely [0.431, 0.107, 0.339, 0.018, 0.105] for class [Normal, Probe, DoS, U2R, R2L] respectively.
%Besides, we also calculate the confusion matrices of the classification results when applying
%different approaches on both task's test datasets.
%In a confusion matrix table, the $i$th row represents the instances of class $i$,
%while the $j$th column represents the instances predicted by the classifier as class $j$.
%It is called confusion matrix because it is useful for visualizing how a classifier
%is confusing one class with other classes.
%Due to page space limit, here we only present the most straightforward
%and relatively more important metric accuracy.
%Statistics regarding precision, recall and confusion matrices can be found
%in our detailed technical report~\cite{OurWonReport} and our codebase~\cite{NetLearner}.

For comparison, we train a RBF-kernel SVM using~\cite{ScikitLearnSVM} and report its accuracy along
with multilayer perceptron (MLP), restricted Boltzmann machine finetuned neural network (RBM),
sparse autoencoder finetuned neural network (SAE) and wide linear classifier and deep neural network combined model (WnD).

Before training machine learning models, research need to take a great effort to search
the optimal hyper-parameters that fit both problem domain and model.
In our case, we manually set all the hyper-parameters, e.g. number of layers, number of neurons
in each layers, learning rate, batch size, to be the same across all considered models such that
it is easier to compare them.
We then use 5-fold cross-validation on the training datasets to determine the optimal training time $T$
for each models.
At last, we train the model for $T$ epochs and report the metrics on the testing dataset,
which is never touched during training phase.
In this way, we have fixed the complexity of the compared models.
The accuracies of each classifier on test datasets for both tasks are
shown in Figure~\ref{Fig:CompAccuracyNSL} and Figure~\ref{Fig:CompAccuracyUNSW} respectively.
In the 2-class UNSW-NB15 dataset,
the amount of normal and attacking traffic are nearly balanced (37,000 normal v.s. 45,332 attacking records).
Therefore we only report the precision and recall for the attacking traffics in Figure~\ref{Fig:CompAccuracyUNSW}.
For the 5-class NSL-KDD task, we adopt the approach in~\cite{STL-NIDS}
to calculate the weighted precisions and recalls and plot them along with accuracy in Figure~\ref{Fig:CompAccuracyNSL}.
The per-class precisions and recalls are also separately listed in Table~\ref{Tab:PrecisionRecall}.

For the NSL-KDD task, all classifiers have achieved very high training accuracy (no less than 99\%, not reported in figures).
However, none of the classifiers do a great job on the testing data;
there is a gap between training accuracy and testing accuracy (as low as 78.4\%).
As the representative of the classic machine learning approach, SVM achieves a 78.5\% accuracy
comparable to the deep learning models.
Our SAE achieves similar accuracy performance to~\cite{STL-NIDS}, which is
also the best among all the considered models (79.2\%).
RBM, SAE and WnD all outperform MLP, but for two different reasons.
Both RBM and SAE provide their underlying MLP with better initial weights in the first layer
than randomly generated numbers.
WnD, on the other hand, has a slight higher accuracy because of the extra linear model.
Table~\ref{Tab:PrecisionRecall} shows two remarkable facts.
SVM is better at Probe attacks (93\% precision and 82\% recall)
than all neural networks ($\approx$ 85\% precision and $\leq$ 70\% recall),
However, it suffers extremely on U2R attacks ($\approx$ 6\% precision and recall).
Neural networks (MLP, RBM, SAE and WnD), though missing a lot of U2R attacks ($\leq$ 6\% recall),
have much greater reliability in identifying these attacking traffics ($\approx$ 60\% precision).

For the UNSW-NB15 we see similar phenomenon: the average accuracies of RBM, SAE and WnD are
all higher than MLP, for the same reason in NSL-KDD task.
We notice WnD has significantly improved MLP's performance by around 5\%.
Different from NSL-KDD task, the training accuracies of all the approaches (not reported in figures) are mediocre,
with maximum at 94.4\%, in contrast to NSL-KDD case where everyone has higher than 99\% training accuracy.
The lower UNSW-NB15 training accuracies, we believe, are mostly because its training dataset
is more difficult than NSL-KDD training dataset.
Harder training dataset is probably one of the reasons that testing accuracies for UNSW-NB15 task
are commonly higher than the testing accuracies for NSL-KDD task,
since classifier only have access to training dataset.
In this situation, even though SVM has similar training accuracy to neural networks (93\% v.s. 94\%),
its testing accuracy falls far behind by 5\% (comparing to MLP) to 9\% (comparing to WnD),
showing the superior generalization capability of deep neural network models.
The detection alarms are mostly trustable for every classifier,
due to very high recall values $\geq$ 97\%,
while deep learning models commonly have better precision ($\geq$ 81\%) than SVM (75\%).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/comp_accuracy_nsl.pdf}
    \caption{Metric Comparison on NSL-KDD Task}
    \label{Fig:CompAccuracyNSL}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/comp_accuracy_unsw.pdf}
    \caption{Metric Comparison of on UNSW-NB15 Task}
    \label{Fig:CompAccuracyUNSW}
\end{figure}

\begin{table}[]
    \centering
    \caption{Per-class Precision/Recall on NSL-KDD Task}
    \label{Tab:PrecisionRecall}
    \begin{tabular}{c|c|ccccc}
        \hline
        \hline
                             &            & \multicolumn{5}{c}{Traffic Class} \\
        \cline{3-7}
                             &            & Normal & DoS   & Probe & U2R   & R2L \\
        \hline
        \multirow{2}{*}{SVM} & Precision  & 72.82  & 75.27 & 93.85 &  6.32 & 96.65 \\
        \cline{2-2}
                             & Recall     & 96.18  & 72.16 & 82.16 &  6.06 & 13.34 \\
        \hline
        \multirow{2}{*}{MLP} & Precision  & 69.18  & 95.49 & 85.58 & 59.52 & 92.01 \\
        \cline{2-2}
                             & Recall     & 96.66  & 82.31 & 69.28 &  6.31 & 15.03 \\
        \hline
        \multirow{2}{*}{RBM} & Precision  & 69.41  & 95.41 & 85.23 & 41.51 & 93.86 \\
        \cline{2-2}
                             & Recall     & 96.81  & 83.89 & 64.74 &  5.56 & 15.48 \\
        \hline
        \multirow{2}{*}{SAE} & Precision  & 70.20  & 95.63 & 84.70 & 65.00 & 87.76 \\
        \cline{2-2}
                             & Recall     & 96.92  & 83.34 & 70.56 &  3.28 & 16.29 \\
        \hline
        \multirow{2}{*}{WnD} & Precision  & 70.08  & 95.59 & 84.02 & 60.34 & 91.57 \\
        \cline{2-2}
                             & Recall     & 96.88  & 83.64 & 67.84 &  4.53 & 15.34 \\
        \hline
    \end{tabular}
\end{table}

